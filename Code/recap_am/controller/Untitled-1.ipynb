{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\recap-am-71VLKO9w-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from nltk import PunktSentenceTokenizer\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import numpy as np\n",
    "from spacy.language import Language\n",
    "\n",
    "from recap_am.controller.extract_features import set_features\n",
    "from recap_am.controller.nlp import parse\n",
    "from recap_am.model.config import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config.get_instance()\n",
    "lang = config[\"nlp\"][\"language\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"&nbsp;[a-zA-Z0-9]?\", \"\", text)\n",
    "    text = (\n",
    "        text.replace(\"Art.\", \"Artikel\")\n",
    "        .replace(\"Abs.\", \"Absatz\")\n",
    "        .replace(\"u.a.\", \"unter anderem\")\n",
    "        .replace(\"U.a.\", \"Unter anderem\")\n",
    "        .replace(\"u.E.\", \"unseres Erachtens\")\n",
    "        .replace(\"U.E.\", \"Unseres Erachtens\")\n",
    "        .replace(\"vgl.\", \"vergleiche\")\n",
    "        .replace(\"Vgl.\", \"Vergleiche\")\n",
    "        .replace(\"bzw.\", \"beziehungsweise\")\n",
    "        .replace(\"i.V.m.\", \"im Vergleich mit\")\n",
    "        .replace(\"Buchst.\", \"Buchstabe\")\n",
    "        .replace(\"d.h.\", \"das heißt\")\n",
    "        .replace(\"'\", \"\")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\";\", \"\")\n",
    "    )\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,?!äÄöÖüÜ:;&ß%$'\\\"()[\\]{} -]\\n\", \"\", text)\n",
    "    text = text.replace(\"...\", \"\")\n",
    "    text = re.sub(r\" +\", \" \", text)\n",
    "    text = text.strip(\" \")\n",
    "    return text\n",
    "\n",
    "def add_labels(doc, labels):\n",
    "    \"\"\"Ajoute les étiquettes à partir d'une liste.\"\"\"\n",
    "    adu_labels_list = []\n",
    "    clpr_label_list = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        label = label.strip(\"\\n\").strip(\" \")\n",
    "        if label == \"Claim\":\n",
    "            adu_labels_list.append(1)\n",
    "            clpr_label_list.append(1)\n",
    "        elif label == \"Premise\":\n",
    "            adu_labels_list.append(1)\n",
    "            clpr_label_list.append(0)\n",
    "        elif label == \"MajorClaim\":\n",
    "            adu_labels_list.append(1)\n",
    "            clpr_label_list.append(1)\n",
    "        elif label == \"None\":\n",
    "            adu_labels_list.append(0)\n",
    "        elif label == \"ADU\":\n",
    "            adu_labels_list.append(1)\n",
    "        elif label == \"1\":\n",
    "            adu_labels_list.append(1)\n",
    "        elif label == \"0\":\n",
    "            adu_labels_list.append(0)\n",
    "    if len(adu_labels_list) > len(doc._.Features):\n",
    "        adu_labels_list = adu_labels_list[: len(doc._.Features)]\n",
    "    elif len(adu_labels_list) < len(doc._.Features):\n",
    "        add_on = np.random.randint(low=0, high=1, size=len(doc._.Features) - len(adu_labels_list)).tolist()\n",
    "        adu_labels_list.extend(add_on)\n",
    "    nr_adus = sum([1 for l in adu_labels_list if l == 1])\n",
    "    if len(clpr_label_list) > nr_adus:\n",
    "        clpr_label_list = clpr_label_list[:nr_adus]\n",
    "    elif len(clpr_label_list) < nr_adus:\n",
    "        add_on = np.random.randint(low=0, high=1, size=nr_adus - len(clpr_label_list)).tolist()\n",
    "        clpr_label_list.extend(add_on)\n",
    "    doc._.Labels = adu_labels_list\n",
    "    doc._.CLPR_Labels = clpr_label_list\n",
    "    return doc\n",
    "\n",
    "def prep_training(filename, input_text, labels_list):\n",
    "    doc = parse(input_text)\n",
    "    doc._.key = filename\n",
    "    doc = set_features(doc)\n",
    "    doc = add_labels(doc, labels_list)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_in(file_name1, file_name2, texts, label_list):\n",
    "    if os.path.isfile(file_name1):\n",
    "        with open(file_name1, \"r+\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "        with open(file_name2, \"r+\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "            labels = f.read().split(\"\\n\")\n",
    "    else:\n",
    "        with open(config[\"adu\"][\"path\"][\"input\"] + \"/\" + file_name1, \"r+\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "            text = f.read()\n",
    "        with open(config[\"adu\"][\"path\"][\"label\"] + \"/\" + file_name2, \"r+\", encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "            labels = f.read().split(\"\\n\")\n",
    "    text = clean_text(text)\n",
    "    texts.append(text)\n",
    "    label_list.append(labels)\n",
    "\n",
    "def merge_docs(doc_list):\n",
    "    comb_feat = list(itertools.chain.from_iterable(list(map(lambda x: x._.Features, doc_list))))\n",
    "    comb_label = list(itertools.chain.from_iterable(list(map(lambda x: x._.Labels, doc_list))))\n",
    "    comb_clpr_label = list(itertools.chain.from_iterable(list(map(lambda x: x._.CLPR_Labels, doc_list))))\n",
    "    comb_embedding = list(itertools.chain.from_iterable(list(map(lambda x: x._.embeddings, doc_list))))\n",
    "    final_text = \"FinalDocument\"\n",
    "    final_doc = parse(final_text)\n",
    "    final_doc._.Features = comb_feat\n",
    "    final_doc._.Labels = comb_label\n",
    "    final_doc._.CLPR_Labels = comb_clpr_label\n",
    "    final_doc._.embeddings = comb_embedding\n",
    "    print(\"Merged Lists\")\n",
    "    return final_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Initialiser localement les listes partagées\n",
    "if config[\"debug\"]:\n",
    "    texts = []\n",
    "    label_list = []\n",
    "    print(\"h\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_list=\"C:\\\\Users\\\\DELL\\\\argument-graph-mining\\\\data\\\\ADU\\\\in\\\\PE\\\\en\\train\\\\micro_c004.text\"\n",
    "label=\"C:\\\\Users\\\\DELL\\\\argument-graph-mining\\\\data\\\\ADU\\\\in\\\\PE\\\\en\\\\train\\\\micro_c004.label\"\n",
    "manager = multiprocessing.Manager()\n",
    "texts = manager.list()\n",
    "label_list = manager.list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(input_list, list) or isinstance(input_list, tuple):\n",
    "    jobs = []\n",
    "    for idx, infile in enumerate(input_list):\n",
    "        print(\"Reading Document\\t%s\" % infile)\n",
    "        p = multiprocessing.Process(target=read_in, args=(infile, label[idx], texts, label_list))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "    doc_list = []\n",
    "    for idx, doc in enumerate(\n",
    "        parse.pipe(\n",
    "            texts,\n",
    "            disable=[\"ner\"],\n",
    "            batch_size=80,\n",
    "            n_process=multiprocessing.cpu_count(),\n",
    "        )\n",
    "    ):\n",
    "        print(\"Processing Document\\t%i\" % idx)\n",
    "        doc._.key = input_list[idx]\n",
    "        doc = set_features(doc)\n",
    "        doc = add_labels(doc, label_list[idx])\n",
    "        doc_list.append(doc)\n",
    "    final_doc = merge_docs(doc_list)\n",
    "else:\n",
    "    with open(input_list, \"r+\", encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "    text = clean_text(text)\n",
    "    with open(label, \"r+\", encoding=\"utf8\") as f:\n",
    "        labels = f.read().split(\"\\n\")\n",
    "    final_doc = prep_training(input_list, text, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=[]\n",
    "label_list=[]\n",
    "file1=\"C:\\\\Users\\\\DELL\\\\argument-graph-mining\\\\data\\\\ADU\\\\in\\\\PE\\\\en\\train\\\\micro_c004.text\"\n",
    "file2=\"C:\\\\Users\\\\DELL\\\\argument-graph-mining\\\\data\\\\ADU\\\\in\\\\PE\\\\en\\\\train\\\\micro_c004.label\"\n",
    "read_in(file1,file2,texts,label_list)\n",
    "print(texts,label_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recap-am-71VLKO9w-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
